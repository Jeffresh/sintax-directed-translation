{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sintax directed translate\n",
    "\n",
    "Design translation scheme (offset parameters Pascal style. That is, the parameters are put on the stack form left ro right)\n",
    "\n",
    "### Grammar:\n",
    "\n",
    "```\n",
    "Def -> ID ID Resto\n",
    "Resto -> ‘,’ Tipo ID Resto\n",
    "Resto -> epsilon\n",
    "Tipo -> INT\n",
    "Tipo -> CHAR\n",
    "Tipo -> FLOAT\n",
    "```\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "`f(int a, float b, char c)`\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Offset de c = 4\n",
    "Offset de b = 4 + sizeof(char)\n",
    "Offset de a = 4 + sizeof(char) + sizeof(float)\n",
    "\n",
    "```\n",
    "\n",
    "# Implementing the lexical analyzer\n",
    "\n",
    "First we have to import the lexer from sly:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sly import Lexer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we have to define our lexical analyzer, creating a class that inherits from Lexer and define the tokens and literals.\n",
    "To avoid the errors from blank characters, add it to ignore variable (in this case we will ignore blank spaces and tabs).\n",
    "ID is defined using a *re* so be careful with that because that definition matches the types. So you have to add\n",
    "the special as special cases of *ID* definitions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class Scanner(Lexer):\n",
    "    tokens = {ID, TIPO}\n",
    "    literals = {','}\n",
    "\n",
    "    #ignore whitespace and tabulation\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expression rules for tokens\n",
    "\n",
    "    ID = r'[a-zA-Z][\\w_]*'\n",
    "\n",
    "    # special cases\n",
    "\n",
    "    ID['int'] = TIPO\n",
    "    ID['float'] = TIPO\n",
    "    ID['char'] = TIPO\n",
    "\n",
    "    # Error handling rule\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character {}\".format(t.value[0]))\n",
    "        self.index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test our Lexer analyzer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type = TIPO, value = int\n",
      "type = ID, value = a\n",
      "type = ,, value = ,\n",
      "type = TIPO, value = float\n",
      "type = ID, value = b\n",
      "type = ,, value = ,\n",
      "type = TIPO, value = char\n",
      "type = ID, value = c\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = \"int a, float b, char c\"\n",
    "    lexer = Scanner()\n",
    "    for token in lexer.tokenize(data):\n",
    "        print('type = {}, value = {}'.format(token.type, token.value))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}