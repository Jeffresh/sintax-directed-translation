{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sintax directed translate\n",
    "\n",
    "Design translation scheme (offset parameters Pascal style. That is, the parameters are put on the stack form left ro right)\n",
    "\n",
    "### Grammar:\n",
    "\n",
    "```\n",
    "Def -> ID Tipo Resto\n",
    "Resto -> ‘,’ Tipo ID Resto\n",
    "Resto -> epsilon\n",
    "Tipo -> INT\n",
    "Tipo -> CHAR\n",
    "Tipo -> FLOAT\n",
    "```\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "`f(int a, float b, char c)`\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Offset de c = 4\n",
    "Offset de b = 4 + sizeof(char)\n",
    "Offset de a = 4 + sizeof(char) + sizeof(float)\n",
    "\n",
    "```\n",
    "\n",
    "# Design translation scheme\n",
    "\n",
    "For every rule we have to define the semantic ones. That rules will be the actions and the returns of our functions. The headers of the grammar rules\n",
    "will be the function names and the body of the rules will be the decorator of that functions that works likes a re in SLY sintaxis.\n",
    "\n",
    "| Sintactic Rules            | Semantic Rules                                                              |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| Def - > ID Tipo Resto      | write('Offset of {tipo.s} + resto.s')                                       |\n",
    "| Resto -> ',' Tipo ID Resto | write('Offset of tipo.s = 4 + resto.s') and resto.s = resto.s + sizeof(tipo.s) |\n",
    "| Resto -> Epsilon           | resto.s = ''                                                                |\n",
    "| Tipo -> Int                | tipo.s := Int.lexval                                                               |\n",
    "| Tipo -> Float              | tipo.s := Float.lexval                                                             |\n",
    "| Tipo -> Char               | tipo.s := Char.lexval                                                              |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "source": [
    "# Implementing the lexical analyzer (Lexer)\n",
    "\n",
    "First we have to import the lexer from sly:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sly import Lexer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we have to define our lexical analyzer, creating a class that inherits from Lexer and define the tokens and literals.\n",
    "To avoid the errors from blank characters, add it to ignore variable (in this case we will ignore blank spaces and tabs).\n",
    "ID is defined using a *re* so be careful with that because that definition matches the types. So you have to add\n",
    "the special as special cases of *ID* definitions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Scanner(Lexer):\n",
    "    tokens = {ID, INT, FLOAT, CHAR}\n",
    "    literals = {','}\n",
    "\n",
    "    #ignore whitespace and tabulation\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    # Regular expression rules for tokens\n",
    "\n",
    "    ID = r'[a-zA-Z][\\w_]*'\n",
    "\n",
    "    # special cases\n",
    "\n",
    "    ID['int'] = INT\n",
    "    ID['float'] = FLOAT\n",
    "    ID['char'] = CHAR\n",
    "\n",
    "    # Error handling rule\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character {}\".format(t.value[0]))\n",
    "        self.index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test our Lexer analyzer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "type = INT, value = int\ntype = ID, value = a\ntype = ,, value = ,\ntype = FLOAT, value = float\ntype = ID, value = b\ntype = ,, value = ,\ntype = CHAR, value = char\ntype = ID, value = c\n"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = \"int a, float b, char c\"\n",
    "    lexer = Scanner()\n",
    "    for token in lexer.tokenize(data):\n",
    "        print('type = {}, value = {}'.format(token.type, token.value))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing syntactic analyzer (Parser)\n",
    "\n",
    "To implement the grammar rules, we will implement a Parser first import Parser from sly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sly import Parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will do the same that we do to implement the Lexer, create a new class that inherits from Parser, pass the tokens\n",
    "from the lexer to our parser, and define the grammar rules."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class FunctionParser(Parser):\n",
    "    tokens = Scanner.tokens\n",
    "\n",
    "    @_('tipo ID resto')\n",
    "    def def_(self, p):\n",
    "        print('Offset de {} = 4'.format(p.ID) + p.resto)\n",
    "\n",
    "    @_('\",\" tipo ID resto')\n",
    "    def resto(self, p):\n",
    "        print('Offset de {} = 4'.format(p.ID) + p.resto)\n",
    "        return p.resto +' + sizeof({})'.format(p.tipo)\n",
    "    @_('')\n",
    "    def resto(self, p):\n",
    "        return ''\n",
    "    @_('INT', 'FLOAT', 'CHAR')\n",
    "    def tipo(self, p):\n",
    "        return p[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing the parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Offset de c = 4\nOffset de b = 4 + sizeof(char)\nOffset de a = 4 + sizeof(char) + sizeof(float)\n"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lexer = Scanner()\n",
    "    parser = FunctionParser()\n",
    "    string = \"int a, float b, char c\"\n",
    "    parser.parse(lexer.tokenize(string))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('venv')",
   "display_name": "Python 3.8.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "86f5510f37a73bccba776b29d689d77308b2249fed85d6de7b84d5b4c47e30c2"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}